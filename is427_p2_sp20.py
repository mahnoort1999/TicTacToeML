# -*- coding: utf-8 -*-
"""IS427_P2_Sp20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OMcOkwkuKvWbKDc1y7-jxDdN3sCr9qa1
"""

#Mahnoor Tariq
#IS 427 - IP 2

#imports into program to use np, pd, and sklearn
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
#from sklearn.impute import SimpleImputer
#from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc

#Import .data file and rename the columns
train = pd.read_csv("ip2ttt_train.data", sep = ",")
train.columns = ["first_row_left", "first_row_middle", "first_row_right", 
                 "center_row_left", "center_row_middle", "center_row_right", 
                 "bottom_row_left", "bottom_row_middle", "bottom_row_right", "is_win"]
train.head()

#Import .data file and rename the columns
test = pd.read_csv("ip2ttt_test.data", sep = ",")
test.columns = ["first_row_left", "first_row_middle", "first_row_right", 
                "center_row_left", "center_row_middle", "center_row_right", 
                "bottom_row_left", "bottom_row_middle", "bottom_row_right", "is_win"]
test.head()

#Import .data file and rename the columns
valid = pd.read_csv("ip2ttt_valid.data", sep = ",")
valid.columns = ["first_row_left", "first_row_middle", "first_row_right", 
                 "center_row_left", "center_row_middle", "center_row_right", 
                 "bottom_row_left", "bottom_row_middle", "bottom_row_right", "is_win"]
valid.head()

#Create dataframe and use get_dummies to change to 0s and 1s
df_train = pd.DataFrame(train)
train_getdummies = pd.get_dummies(df_train)
train_getdummies

#Create dataframe and use get_dummies to change to 0s and 1s
df_test = pd.DataFrame(test)
test_getdummies = pd.get_dummies(df_test)
test_getdummies

#Create dataframe and use get_dummies to change to 0s and 1s
df_valid = pd.DataFrame(valid)
valid_getdummies = pd.get_dummies(df_valid)
valid_getdummies

#Merge dummy data with original data files
merged_train = pd.concat([df_train, train_getdummies], axis = "columns")
merged_train

#Merge dummy data with original data files
merged_test = pd.concat([df_test, test_getdummies], axis = "columns")
merged_test

#Merge dummy data with original data files
merged_valid = pd.concat([df_valid, valid_getdummies], axis = "columns")
merged_valid

#For all the values within the 'is_win' column, assign 0 or 1 depending on positive or negative
train.loc[train['is_win'] == 'positive', 'is_win'] = 1
train.loc[train['is_win'] == 'negative', 'is_win'] = 0

train['is_win'].unique()

#For all the values within the 'is_win' column, assign 0 or 1 depending on positive or negative
test.loc[test['is_win'] == 'positive', 'is_win'] = 1
test.loc[test['is_win'] == 'negative', 'is_win'] = 0

test['is_win'].unique()

#For all the values within the 'is_win' column, assign 0 or 1 depending on positive or negative
valid.loc[valid['is_win'] == 'positive', 'is_win'] = 1
valid.loc[valid['is_win'] == 'negative', 'is_win'] = 0

valid['is_win'].unique()

#drop original data columns in .data files and keeps columns with dummy data
final_train = merged_train.drop(["first_row_left", "first_row_middle", "first_row_right", 
                                 "center_row_left", "center_row_middle", "center_row_right", 
                                 "bottom_row_left", "bottom_row_middle", "bottom_row_right", 
                                 "is_win_negative", "is_win_positive"], axis = "columns")
final_train

#drop original data columns in .data files and keeps columns with dummy data
final_test = merged_test.drop(["first_row_left", "first_row_middle", "first_row_right", 
                               "center_row_left", "center_row_middle", "center_row_right", 
                               "bottom_row_left", "bottom_row_middle", "bottom_row_right", 
                               "is_win_negative", "is_win_positive"], axis = "columns")
final_test

#drop original data columns in .data files and keeps columns with dummy data
final_valid = merged_valid.drop(["first_row_left", "first_row_middle", "first_row_right", 
                                 "center_row_left", "center_row_middle", "center_row_right", 
                                 "bottom_row_left", "bottom_row_middle", "bottom_row_right", 
                                 "is_win_negative", "is_win_positive"], axis = "columns")
final_valid

#LabelEncoder is used to initilize y-values as 0s and 1s using the dataframe made before
le_train = LabelEncoder()
dfle_train = final_train
dfle_train.is_win = le_train.fit_transform(dfle_train.is_win)
y_train = dfle_train.is_win
y_train

#LabelEncoder is used to initilize y-values as 0s and 1s using the dataframe made before
le_test = LabelEncoder()
dfle_test = final_test
dfle_test.is_win = le_test.fit_transform(dfle_test.is_win)
y_test = dfle_test.is_win
y_test

#LabelEncoder is used to initilize y-values as 0s and 1s using the dataframe made before
le_valid = LabelEncoder()
dfle_valid = final_valid
dfle_valid.is_win = le_valid.fit_transform(dfle_valid.is_win)
y_valid = dfle_valid.is_win
y_valid

#Initilize the x-values 
x_train = dfle_train[["first_row_left_o", "first_row_left_x",
                    "first_row_middle_o", "first_row_middle_x",
                    "first_row_right_o", "first_row_right_x",
                    "center_row_left_o", "center_row_left_x",
                    "center_row_middle_o", "center_row_middle_x",
                    "center_row_right_o", "center_row_right_x", 
                    "bottom_row_left_o", "bottom_row_left_x",
                    "bottom_row_middle_o", "bottom_row_middle_x",
                    "bottom_row_right_o", "bottom_row_right_x"]].values
x_train

#Initilize the x-values 
x_test = dfle_test[["first_row_left_o", "first_row_left_x",
                    "first_row_middle_o", "first_row_middle_x",
                    "first_row_right_o", "first_row_right_x",
                    "center_row_left_o", "center_row_left_x",
                    "center_row_middle_o", "center_row_middle_x",
                    "center_row_right_o", "center_row_right_x", 
                    "bottom_row_left_o", "bottom_row_left_x",
                    "bottom_row_middle_o", "bottom_row_middle_x",
                    "bottom_row_right_o", "bottom_row_right_x"]].values
x_test

#Initilize the x-values 
x_valid = dfle_valid[["first_row_left_o", "first_row_left_x",
                    "first_row_middle_o", "first_row_middle_x",
                    "first_row_right_o", "first_row_right_x",
                    "center_row_left_o", "center_row_left_x",
                    "center_row_middle_o", "center_row_middle_x",
                    "center_row_right_o", "center_row_right_x", 
                    "bottom_row_left_o", "bottom_row_left_x",
                    "bottom_row_middle_o", "bottom_row_middle_x",
                    "bottom_row_right_o", "bottom_row_right_x"]].values
x_valid

#Initizile the y-values
y_train = train['is_win'].astype('int')
y_train

#Initizile the y-values
y_test = test['is_win'].astype('int')
y_test

#Initizile the y-values
y_valid = valid['is_win'].astype('int')
y_valid

#Preprocessing Done. Values here are used later on.
#test-size: 0.3 for testing and 0.7 for training.
X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=42)

#Logistic Regression with l2 and newton-cg
lr_newton = LogisticRegression(penalty= 'l2', solver='newton-cg')
lr_newton.fit(X_train, y_train)

#Logistic Regression with l2 and lbfgs
lr_lbfgs = LogisticRegression(penalty='l2', solver ='lbfgs')
lr_lbfgs.fit(X_train, y_train)

#DOES NOT WORK: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.
lr_lbfgs = LogisticRegression(penalty='l1', solver ='lbfgs')
lr_lbfgs.fit(X_train, y_train)

#DOES NOT WORK: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.
lr_lbfgs = LogisticRegression(penalty='l1', solver ='newton-cg')
lr_lbfgs.fit(X_train, y_train)

#Predict the values using X_train and X_test for newton-cg
pred_newton = lr_newton.predict(X_train)
print("\nLogistic Regression - Train accuracy (Eye Detection)", round(accuracy_score(y_train, pred_newton), 5))

pred_newton = lr_newton.predict(X_test)
print("\nLogistic Regression - Test accuracy (Eye Detection)", round(accuracy_score(y_test, pred_newton), 5))

#Predict the values using X_train and X_test for lbfgs
pred_lbfgs = lr_lbfgs.predict(X_train)
print("\nLogistic Regression - Train accuracy (Eye Detection)", round(accuracy_score(y_train, pred_lbfgs), 5))

pred_lbfgs = lr_lbfgs.predict(X_test)
print("\nLogistic Regression - Train accuracy (Eye Detection)", round(accuracy_score(y_test, pred_lbfgs), 5))

#Decision Tree Classifier
#Criterion is = 'entropy' for all. Keep changing min_samples_leaf values from 1-10
model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 1)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 2)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 3)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 4)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 5)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 6)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 7)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 8)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 9)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

model_dtc = DecisionTreeClassifier(criterion = "entropy", min_samples_leaf = 10)
model_dtc.fit(X_train, y_train)
prediction_dtc = model_dtc.predict(X_test)
print('Decison Tree - Test accuracy: ', accuracy_score(prediction_dtc, y_test))

#KNeighborsClassifier
#Change the number of n_neighbors --> 1, 3, 5, 7, 9
clf3 = KNeighborsClassifier(n_neighbors=1)
clf3.fit(X_train, y_train) #training first
k_score = clf3.score(X_test, y_test) #use testing variables
k_score

k_score_valid = clf3.score(x_valid, y_valid) #use validation variables
k_score_valid

clf3 = KNeighborsClassifier(n_neighbors=3)
clf3.fit(X_train, y_train) #training first 
k_score = clf3.score(X_test, y_test) #use testing variables
k_score

k_score_valid = clf3.score(x_valid, y_valid) #use validation variables
k_score_valid

clf3 = KNeighborsClassifier(n_neighbors=5)
clf3.fit(X_train, y_train) #training first
k_score = clf3.score(X_test, y_test) #use testing variables
k_score

k_score_valid = clf3.score(x_valid, y_valid) #use validation variables
k_score_valid

clf3 = KNeighborsClassifier(n_neighbors=7)
clf3.fit(X_train, y_train) #training first
k_score = clf3.score(X_test, y_test) #use testing variables
k_score

k_score_valid = clf3.score(x_valid, y_valid) #use validation variables
k_score_valid

clf3 = KNeighborsClassifier(n_neighbors=9)
clf3.fit(X_train, y_train) #training first
k_score = clf3.score(X_test, y_test) #use testing variables
k_score

k_score_valid = clf3.score(x_valid, y_valid) #use validation variables
k_score_valid

#RandomForest Classifier 
#Change max_depth and min_samples_leaf values
rf = RandomForestClassifier(max_depth=3, min_samples_leaf=5)
rf.fit(X_train, y_train) #training first
rf_predict = rf.predict(X_test) #predict using X_test variable

#AUC - Area Under the Curve
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, rf_predict) #use prediction output and y_test variable
roc_auc = auc(false_positive_rate, true_positive_rate)
roc_auc

rf_predict = rf.predict(x_valid) #use validation variable 
rf_predict

#RandomForest Classifier 
#Change max_depth and min_samples_leaf values
rf = RandomForestClassifier(max_depth=5, min_samples_leaf=10)
rf.fit(X_train, y_train)
rf_predict = rf.predict(X_test)

#AUC - Area Under the Curve
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, rf_predict) #use prediction output and y_test variable
roc_auc = auc(false_positive_rate, true_positive_rate)
roc_auc

rf_predict = rf.predict(x_valid) #use validation variable 
rf_predict

#RandomForest Classifier 
#Change max_depth and min_samples_leaf values
rf = RandomForestClassifier(max_depth=3, min_samples_leaf=10)
rf.fit(X_train, y_train)
rf_predict = rf.predict(X_test)

#AUC - Area Under the Curve
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, rf_predict) #use prediction output and y_test variable
roc_auc = auc(false_positive_rate, true_positive_rate)
roc_auc

rf_predict = rf.predict(x_valid) #use validation variable 
rf_predict

#RandomForest Classifier 
#Change max_depth and min_samples_leaf values
rf = RandomForestClassifier(max_depth=5, min_samples_leaf=10)
rf.fit(X_train, y_train)
rf_predict = rf.predict(X_test)

#AUC - Area Under the Curve
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, rf_predict) #use prediction output and y_test variable
roc_auc = auc(false_positive_rate, true_positive_rate)
roc_auc

rf_predict = rf.predict(x_valid) #use validation variable 
rf_predict